{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Run the next cell\n",
        "\n",
        "you need T4 gpu to be able to test the model"
      ],
      "metadata": {
        "id": "ToPD6zvQuXCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXzIEAZRuR0Q"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Tacotron and Waveglow (click to see code)\n",
        "\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "!pip install gdown\n",
        "git_repo_url = 'https://github.com/maxmelichov/tacotron2.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # clone and install\n",
        "  !git clone -q --recursive {git_repo_url}\n",
        "  !cd {project_name}/waveglow && git checkout 2fd4e63\n",
        "  !pip install -q librosa unidecode\n",
        "  !pip install Hebrew\n",
        "  \n",
        "import sys\n",
        "sys.path.append(join(project_name, 'waveglow/'))\n",
        "sys.path.append(project_name)\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "import gdown\n",
        "from hebrew import Hebrew\n",
        "from hebrew.chars import HebrewChar, ALEPH\n",
        "from hebrew import GematriaTypes\n",
        "d = 'https://drive.google.com/uc?id='"
      ],
      "metadata": {
        "id": "NatDNxccuUmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pre-trained model won't be shared until the end of the contest"
      ],
      "metadata": {
        "id": "TaPJp2QNut8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "force_download_TT2 = True\n",
        "tacotron2_pretrained_model = 'MLPTTS'\n",
        "if not exists(tacotron2_pretrained_model) or force_download_TT2:\n",
        "                   # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ PUT MODEL HERE\n",
        "  gdown.download(d+r'None', tacotron2_pretrained_model, quiet=False); print(\"Tacotron2 Model Downloaded\")\n",
        "                   # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ PUT MODEL HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "wgu8WXVkuqtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveglow_pretrained_model = 'waveglow.pt'\n",
        "if not exists(waveglow_pretrained_model):\n",
        "  gdown.download(d+r'1rpK8CzAAirq9sWZhe9nlfvxMF1dRgFbF&export=download', waveglow_pretrained_model, quiet=False); print(\"WaveGlow Model Downloaded\")#1okuUstGoBe_qZ4qUEF8CcwEugHP7GM_b&export"
      ],
      "metadata": {
        "id": "f_eOD7Kyu6bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./tacotron2/waveglow/convert_model.py /content/waveglow.pt /content/waveglow.pt"
      ],
      "metadata": {
        "id": "GKkUcNRsu9sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "locale.getpreferredencoding()"
      ],
      "metadata": {
        "id": "uvqhtbjru_l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Tacotron and Waveglow \n",
        "%matplotlib inline\n",
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from hparams import create_hparams\n",
        "from model import Tacotron2\n",
        "from layers import TacotronSTFT\n",
        "from audio_processing import griffin_lim\n",
        "from text import text_to_sequence\n",
        "from denoiser import Denoiser\n",
        "\n",
        "graph_width = 900\n",
        "graph_height = 360\n",
        "def plot_data(data, figsize=(int(graph_width/100), int(graph_height/100))):\n",
        "    %matplotlib inline\n",
        "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
        "    for i in range(len(data)):\n",
        "        axes[i].imshow(data[i], aspect='auto', origin='upper', \n",
        "                       interpolation='none', cmap='inferno')\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#  the next func is a secret will be shared after the contest\n",
        "def ARPA(sentence,index = 0):\n",
        "    pass\n",
        " \n",
        "\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# initialize Tacotron2 with the pretrained model\n",
        "hparams = create_hparams()"
      ],
      "metadata": {
        "id": "GAhZ-94-vC70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tacotron2 (run this cell every time you change the model)\n",
        "hparams.sampling_rate = 22050 # Don't change this\n",
        "hparams.max_decoder_steps = 1000 # How long the audio will be before it cuts off (1000 is about 11 seconds)\n",
        "hparams.gate_threshold = 0.1 # Model must be 90% sure the clip is over before ending generation (the higher this number is, the more likely that the AI will keep generating until it reaches the Max Decoder Steps)\n",
        "model = Tacotron2(hparams)\n",
        "model.load_state_dict(torch.load(tacotron2_pretrained_model)['state_dict'])\n",
        "_ = model.cuda().eval()"
      ],
      "metadata": {
        "id": "FCyv8MmOvla7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load WaveGlow\n",
        "waveglow = torch.load(waveglow_pretrained_model)['model']\n",
        "waveglow.cuda().eval()\n",
        "for k in waveglow.convinv:\n",
        "    k.float()\n",
        "denoiser = Denoiser(waveglow)"
      ],
      "metadata": {
        "id": "QnW74wETvoIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"בָּנַיי\"\n",
        "sigma = 0.8\n",
        "denoise_strength = 0.1\n",
        "# try to switch raw data to True maybe the results will be better\n",
        "raw_input = False # disables automatic ARPAbet conversion, useful for inputting your own ARPAbet pronounciations or just for testing\n",
        "\n",
        "for i in text.split(\"\\n\"):\n",
        "    if len(i) < 1: continue;\n",
        "    print(i)\n",
        "    if raw_input:\n",
        "        if i[-1] != \";\": i=i+\";\" \n",
        "    else: i = ARPA(i)\n",
        "    print(i)\n",
        "    with torch.no_grad(): # save VRAM by not including gradients\n",
        "        sequence = np.array(text_to_sequence(i, ['english_cleaners']))[None, :]\n",
        "        sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()\n",
        "        mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "        plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],alignments.float().data.cpu().numpy()[0].T))\n",
        "        audio = waveglow.infer(mel_outputs_postnet, sigma=sigma); print(\"\"); ipd.display(ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate))"
      ],
      "metadata": {
        "id": "B5Zb4w8bvog5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}